{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8947e2c-a06e-4f07-a676-f1ea41f1565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training, AdaLoraConfig, AdaLoraConfig\n",
    "\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a578a8-de27-49d5-8ab4-04a57735b585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.isfile('/math_10k.json'))  # This should return True if the file is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15178bec-c1a7-4c59-8bc0-73a69bd9e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the local JSON file\n",
    "raw_datasets = load_dataset('json', data_files='./math_10k.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff6abb6-d878-438a-8252-b648ceca4db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'answer'],\n",
       "        num_rows: 9919\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef74298-2867-4916-b08b-449abae8133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dave initially had 11 tickets. He spent 5 tickets on a beanie, leaving him with:\\n\\n11 - 5 = 6 tickets\\n\\nHe later won 10 more tickets, so his total number of tickets would be:\\n\\n6 + 10 = 16 tickets\\n\\nTherefore, Dave would have 16 tickets after spending 5 tickets on a beanie and winning 10 more tickets. The answer in Arabic numerals is:\\n\\n16'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffa4932-69e4-4868-b369-00e1545abe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "                \n",
    "                ### Input:\n",
    "                {data_point[\"input\"]}\n",
    "                \n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\" # noqa: E501\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  \n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "                \n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\" # noqa: E501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93634b3-de26-45d1-946b-f770058d514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    cutoff_len=512\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a70247-7768-49bb-8971-47bb63a56b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_inputs=True\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = generate_prompt({**data_point, \"output\": \"\"})\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "                                              -100\n",
    "                                          ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                                                                user_prompt_len:\n",
    "                                                                ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e387ff91-2c51-4337-825b-a732cbd40547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Log in using your Hugging Face token\n",
    "login(\"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\")\n",
    "\n",
    "model_name = \"yahma/llama-7b-hf\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.hidden_dropout_prob=0.0\n",
    "config.attention_probs_dropout_prob=0.00\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6d9fbe-54e3-4a78-938f-a5beea4bfeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7d4af7fd8e403ca95913d472e9da85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = raw_datasets[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad153a7-f1c8-45bc-b39a-83fc01688c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7547ec9f-ad18-4053-a57e-9713ef91c833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 9919\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faceac05-23fd-47e6-b4d3-311eec20497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.  \\n\\n                ### Instruction:\\n                 Mike made 14 dollars mowing lawns and 26 dollars weed eating. If he only spent 5 dollar a week, how long would the money last him? \\n \\n                \\n                ### Response:\\n                1. First, we need to find out how much money Mike made in total:\\n    - 14 (dollars from mowing) + 26 (dollars from weed eating) = 40 dollars\\n2. Next, we need to figure out how many weeks the money will last, given that he spends 5 dollars per week:\\n    - 40 (total dollars made) ÷ 5 (dollars spent per week) = 8 weeks\\n3. Therefore, the money will last Mike 8 weeks.\\n\\nAnswer: 8'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_data['input_ids'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c36a1a-afa5-4e03-8f81-c67fad5aaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a497dd-196a-4fef-8296-9807c7ed057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8977910b-c948-4208-a6b5-74e3e9620dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53018913ac0b47e7b54da90003f3a2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f275066c-3a5f-42d4-b1bb-5ed2f24559a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='column', rank=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8435c6b-7f02-404b-8d6d-1962f2e81451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): column()\n",
       "          (k_proj): column()\n",
       "          (v_proj): column()\n",
       "          (o_proj): column()\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): column()\n",
       "          (up_proj): column()\n",
       "          (down_proj): column()\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): column()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f2ae80e-7a76-49dc-8a39-e16c9a67280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters:4441856, percentage:  0.03277787328256677\n"
     ]
    }
   ],
   "source": [
    "# Count of trainable parameters\n",
    "total_trainable_params = 0\n",
    "total =  0\n",
    "# Print trainable parameters and count their total number\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        #print(f\"Parameter name: {name}, Shape: {param.shape}\")\n",
    "        \n",
    "        total_trainable_params += param.numel()\n",
    "    total+=param.numel()\n",
    "\n",
    "print(f\"Total trainable parameters:{total_trainable_params}, percentage:  {total_trainable_params/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e712870-8165-40df-a94b-63e9ed11b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total trainable parameters:3145728, percentage:  0.00046661712910165873 lora\n",
    "#Total trainable parameters:1409024, percentage:  0.010635666584219638 leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b0f8d1d-2b84-477d-b5a0-4f757d49f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f291c573-2dd7-4196-b130-d6e9289fc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f5fd3f8-fd96-4fde-8d34-1ca8268e3f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='659' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 659/1240 1:53:20 < 1:40:14, 0.10 it/s, Epoch 1.06/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.591300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 35\u001b[0m\n\u001b[1;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqnli_dir\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     28\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2021\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2357\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2357\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2360\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2361\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2362\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2363\u001b[0m ):\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3454\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3454\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3459\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3460\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3501\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3500\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3501\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3502\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1185\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1182\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:996\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    984\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    985\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    986\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    993\u001b[0m         position_embeddings,\n\u001b[1;32m    994\u001b[0m     )\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:728\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:613\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    612\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 613\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    616\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/leader.py:63\u001b[0m, in \u001b[0;36mcolumn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     out_trainable \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     out_non_trainable \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk:], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_trainable_weight, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_trainable \u001b[38;5;241m+\u001b[39m out_non_trainable\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='qnli_dir',\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.00,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    gradient_accumulation_steps= 2,\n",
    "\n",
    "    logging_steps=100,\n",
    "   \n",
    "    #load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    #eval_dataset=tokenized_datasets[\"validation\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d62ff1f5-60ed-4e61-b5b6-71158a37b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa221b5052340da9190509983fc37cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140f2d36049443cea03df6141cdff5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_datasets = load_dataset('json', data_files='https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/SingleEq/test.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "66e8579a-32bc-4416-b680-cad2fd0d2c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'answer'],\n",
       "        num_rows: 508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "912fb79a-8eb0-4856-84bc-899000925332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebe6bd439e041acbaa8d38ec2e3e94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = test_datasets[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "292272a9-c7f2-4b0c-9a3c-86e540af7fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 508\n",
       "})"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1a95aff6-ea67-4180-a673-a3caf3b945cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.  \\n\\n                ### Instruction:\\n                Irwin's family went on a camping trip in the mountains. On the first day, they hiked from their car to the campsite. First, they hiked 0.2 of a mile from the car to a stream, and 0.4 of a mile from the stream to a meadow. Then they hiked 0.1 of a mile from the meadow to the campsite. How many miles did Irwin's family hike in all?\\n                \\n                ### Response:\\n                \\nA: They hiked 0.2 of a mile from the car to the stream, 0.4 of a mile from the stream to the meadow, and 0.1 of a mile from the meadow to the campsite. In total, they hiked 0.2 + 0.4 + 0.1 = 0.7 miles. The answer is\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_data['input_ids'][154][:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408ca1b-4fdf-4880-9b94-be9031a93467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce15819f4c984903ab7dcd8d32ba5604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "k = 0\n",
    "ck = 0\n",
    "result = []\n",
    "truth = []\n",
    "for i in tqdm(range(len(test_data['input_ids']))):\n",
    "    input_truth = torch.tensor(test_data['input_ids'][i])\n",
    "    input_ids = torch.tensor(test_data['input_ids'][i][:-5]).unsqueeze(0).to(model.device)\n",
    "    attention_mask = torch.tensor(test_data['attention_mask'][i][:-5]).unsqueeze(0).to(model.device)\n",
    "    #print(input_ids.shape, attention_mask.shape)\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=input_ids.shape[1]+7)\n",
    "\n",
    "    try:\n",
    "        output = tokenizer.decode(output[0]).split('The answer is')[1]\n",
    "        true = tokenizer.decode(input_truth).split('The answer is')[1]\n",
    "        #print(output, float(re.findall(r'\\d+\\.\\d+|\\d+', output)[0])\n",
    "        result.append(float(re.findall(r'\\d+\\.\\d+|\\d+', output)[0]))\n",
    "        truth.append(float(re.findall(r'\\d+\\.\\d+|\\d+', true)[0]))\n",
    "    except:\n",
    "        ck = 1\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9303a3f0-1c24-41a2-9edc-98914f5dffad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21.0,\n",
       " 7790.0,\n",
       " 0.2,\n",
       " 556.0,\n",
       " 507.0,\n",
       " 14.02,\n",
       " 35.52,\n",
       " 1.8333333333333333,\n",
       " 33.52,\n",
       " 76.0,\n",
       " 25.0,\n",
       " 0.6,\n",
       " 65134.0,\n",
       " 0.5,\n",
       " 14.0,\n",
       " 0.25,\n",
       " 7.0,\n",
       " 74.0,\n",
       " 36.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 42.33,\n",
       " 0.625,\n",
       " 5110.0,\n",
       " 0.1,\n",
       " 4.0,\n",
       " 14.02,\n",
       " 23.0,\n",
       " 59.0,\n",
       " 4.0,\n",
       " 0.6666666666666666,\n",
       " 9.43,\n",
       " 42.0,\n",
       " 6.6,\n",
       " 20.66,\n",
       " 109.0,\n",
       " 2454.0,\n",
       " 0.75,\n",
       " 0.875,\n",
       " 14.75,\n",
       " 0.8333333333333334,\n",
       " 7092.0,\n",
       " 0.53,\n",
       " 12.0,\n",
       " 0.4166666666666667,\n",
       " 0.125,\n",
       " 21.0,\n",
       " 15.0,\n",
       " 0.3333333333333334,\n",
       " 11.0,\n",
       " 71.0,\n",
       " 98.0,\n",
       " 0.8888888888888888,\n",
       " 18.0,\n",
       " 3.6666666666666665,\n",
       " 20.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 18.0,\n",
       " 3.0,\n",
       " 0.875,\n",
       " 20.0,\n",
       " 2515.0,\n",
       " 83.0,\n",
       " 84.0,\n",
       " 0.8,\n",
       " 51.0,\n",
       " 19.0,\n",
       " 224.87,\n",
       " 0.8888888888888888,\n",
       " 0.9166666666666666,\n",
       " 18.0,\n",
       " 24.81,\n",
       " 44.0,\n",
       " 25.0,\n",
       " 387.85,\n",
       " 1.0,\n",
       " 48.0,\n",
       " 151.0,\n",
       " 6029.0,\n",
       " 55.0,\n",
       " 0.625,\n",
       " 88.0,\n",
       " 40.17,\n",
       " 35.0,\n",
       " 48.0,\n",
       " 5.666666666666667,\n",
       " 89.0,\n",
       " 212.0,\n",
       " 50.0,\n",
       " 16.0,\n",
       " 270.0,\n",
       " 3.25,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 69.0,\n",
       " 512.0,\n",
       " 12.0,\n",
       " 66.0,\n",
       " 15.1,\n",
       " 1460.0,\n",
       " 0.3,\n",
       " 14696.0,\n",
       " 1792.0,\n",
       " 1.0,\n",
       " 65.0,\n",
       " 74.0,\n",
       " 0.6,\n",
       " 63.0,\n",
       " 3.0,\n",
       " 12.0,\n",
       " 79.0,\n",
       " 7.26,\n",
       " 0.5,\n",
       " 12.3,\n",
       " 7.0,\n",
       " 11.0,\n",
       " 63.0,\n",
       " 6.5,\n",
       " 82.0,\n",
       " 6.0,\n",
       " 44.0,\n",
       " 0.7,\n",
       " 20.52,\n",
       " 5.0,\n",
       " 61.0,\n",
       " 67.0,\n",
       " 10.46,\n",
       " 227.0,\n",
       " 5.0,\n",
       " 48781.0,\n",
       " 11.0,\n",
       " 0.9,\n",
       " 12.0,\n",
       " 971639.0,\n",
       " 14720.0,\n",
       " 12.0,\n",
       " 21.95,\n",
       " 0.08,\n",
       " 10.333333333333332,\n",
       " 11.0,\n",
       " 0.9166666666666666,\n",
       " 5.0,\n",
       " 9.0,\n",
       " 74.0,\n",
       " 3.0,\n",
       " 0.75,\n",
       " 1232.0,\n",
       " 13.0,\n",
       " 0.8333333333333334,\n",
       " 6.0,\n",
       " 21.0,\n",
       " 16.0,\n",
       " 0.6666666666666667,\n",
       " 3.0,\n",
       " 0.5,\n",
       " 0.1,\n",
       " 103.0,\n",
       " 559.0,\n",
       " 42.0,\n",
       " 48.0,\n",
       " 19.02,\n",
       " 27.0,\n",
       " 106491.0,\n",
       " 12.0,\n",
       " 0.8333333333333334,\n",
       " 163.28,\n",
       " 6.0,\n",
       " 3731.0,\n",
       " 0.666666666666666,\n",
       " 5935.0,\n",
       " 25.31,\n",
       " 16.0,\n",
       " 70.0,\n",
       " 9.12,\n",
       " 51.0,\n",
       " 0.3,\n",
       " 118548.0,\n",
       " 0.3333333333333333,\n",
       " 8.0,\n",
       " 0.31,\n",
       " 6279.0,\n",
       " 50.0,\n",
       " 71.0,\n",
       " 574.0,\n",
       " 12.0,\n",
       " 0.8333333333333334,\n",
       " 0.625,\n",
       " 37.0,\n",
       " 141.54,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 52.0,\n",
       " 56.0,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333335,\n",
       " 0.5,\n",
       " 5.88,\n",
       " 9.0,\n",
       " 2713.0,\n",
       " 6.0,\n",
       " 1.22,\n",
       " 16.0,\n",
       " 14.0,\n",
       " 23.0,\n",
       " 7.0,\n",
       " 4.333333333333333,\n",
       " 17.0,\n",
       " 24.0,\n",
       " 19766.0,\n",
       " 6755.0,\n",
       " 9.0,\n",
       " 11687.0,\n",
       " 0.6,\n",
       " 12.7,\n",
       " 72.0,\n",
       " 4.0,\n",
       " 10.666666666666666,\n",
       " 3.0,\n",
       " 469.0,\n",
       " 1.7,\n",
       " 0.3333333333333333,\n",
       " 9.8,\n",
       " 0.9,\n",
       " 18.0,\n",
       " 9.8,\n",
       " 0.9166666666666667,\n",
       " 37.0,\n",
       " 4133.0,\n",
       " 6.0,\n",
       " 59.0,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 47.0,\n",
       " 29.0,\n",
       " 100.0,\n",
       " 342.0,\n",
       " 13.0,\n",
       " 0.7,\n",
       " 0.16666666666666666,\n",
       " 2.0,\n",
       " 0.4,\n",
       " 77.0,\n",
       " 34.0,\n",
       " 252.0,\n",
       " 3.5,\n",
       " 5.0,\n",
       " 18.8,\n",
       " 0.25,\n",
       " 4.0,\n",
       " 17.0,\n",
       " 3120.0,\n",
       " 0.6,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 111421.0,\n",
       " 105.0,\n",
       " 16.0,\n",
       " 78.0,\n",
       " 5.0,\n",
       " 215.0,\n",
       " 13.0,\n",
       " 6.0,\n",
       " 17.0,\n",
       " 104.0,\n",
       " 10.0,\n",
       " 0.52,\n",
       " 41.0,\n",
       " 15.0,\n",
       " 65057.0,\n",
       " 211.0,\n",
       " 26.58,\n",
       " 72.0,\n",
       " 5.333333333333333,\n",
       " 0.75,\n",
       " 8317.0,\n",
       " 13.0,\n",
       " 16.0,\n",
       " 0.8,\n",
       " 0.375,\n",
       " 47.0,\n",
       " 0.46,\n",
       " 1.3333333333333335,\n",
       " 27.3,\n",
       " 0.1,\n",
       " 0.125,\n",
       " 34.72,\n",
       " 0.6,\n",
       " 63.0,\n",
       " 4.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 89.0,\n",
       " 22.0,\n",
       " 77.0,\n",
       " 0.625,\n",
       " 39.0,\n",
       " 10.0,\n",
       " 4.0,\n",
       " 227.0,\n",
       " 16.0,\n",
       " 26.0,\n",
       " 2.75,\n",
       " 12.0,\n",
       " 33.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 43.0,\n",
       " 6.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 4.0,\n",
       " 0.7,\n",
       " 1.3333333333333333,\n",
       " 5855.0,\n",
       " 2.0,\n",
       " 0.09,\n",
       " 242.0,\n",
       " 6.0,\n",
       " 81.0,\n",
       " 83.0,\n",
       " 96.0,\n",
       " 34.0,\n",
       " 50870.0,\n",
       " 67082.0,\n",
       " 6.5,\n",
       " 67.0,\n",
       " 18.0,\n",
       " 3220.0,\n",
       " 0.7,\n",
       " 43.0,\n",
       " 4.6,\n",
       " 523.0,\n",
       " 18.0,\n",
       " 60.0,\n",
       " 57.0,\n",
       " 0.6666666666666666,\n",
       " 27004.0,\n",
       " 0.2,\n",
       " 0.75,\n",
       " 0.2,\n",
       " 55.0,\n",
       " 33.56,\n",
       " 29.0,\n",
       " 0.75,\n",
       " 74.0,\n",
       " 23.86,\n",
       " 0.25,\n",
       " 11.0,\n",
       " 0.2,\n",
       " 452.0,\n",
       " 4.0,\n",
       " 0.5,\n",
       " 488.0,\n",
       " 158.35,\n",
       " 98.0,\n",
       " 10.0,\n",
       " 46.0,\n",
       " 9.0,\n",
       " 0.375,\n",
       " 7.0,\n",
       " 4.0,\n",
       " 90.0,\n",
       " 5703.0,\n",
       " 15.0,\n",
       " 9844.0,\n",
       " 64.0,\n",
       " 217.0,\n",
       " 14.0,\n",
       " 65.0,\n",
       " 63.0,\n",
       " 20.0,\n",
       " 21.93,\n",
       " 117.0,\n",
       " 9.8,\n",
       " 53.0,\n",
       " 40.0,\n",
       " 4.0,\n",
       " 52.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 63.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 17.0]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0555339e-8a24-4f5d-8854-9b2451b87878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21.0,\n",
       " 7790.0,\n",
       " 0.2,\n",
       " 556.0,\n",
       " 507.0,\n",
       " 14.02,\n",
       " 35.52,\n",
       " 1.8333333333333333,\n",
       " 33.52,\n",
       " 76.0,\n",
       " 25.0,\n",
       " 0.6,\n",
       " 65134.0,\n",
       " 0.5,\n",
       " 14.0,\n",
       " 0.25,\n",
       " 7.0,\n",
       " 74.0,\n",
       " 36.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 42.33,\n",
       " 0.625,\n",
       " 5110.0,\n",
       " 0.1,\n",
       " 4.0,\n",
       " 14.02,\n",
       " 23.0,\n",
       " 59.0,\n",
       " 4.0,\n",
       " 0.6666666666666666,\n",
       " 9.43,\n",
       " 42.0,\n",
       " 6.6,\n",
       " 20.66,\n",
       " 109.0,\n",
       " 2454.0,\n",
       " 0.75,\n",
       " 0.875,\n",
       " 14.75,\n",
       " 0.8333333333333334,\n",
       " 7092.0,\n",
       " 0.53,\n",
       " 12.0,\n",
       " 0.4166666666666667,\n",
       " 0.125,\n",
       " 21.0,\n",
       " 15.0,\n",
       " 0.3333333333333334,\n",
       " 11.0,\n",
       " 71.0,\n",
       " 98.0,\n",
       " 0.8888888888888888,\n",
       " 18.0,\n",
       " 3.6666666666666665,\n",
       " 20.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 18.0,\n",
       " 3.0,\n",
       " 0.875,\n",
       " 20.0,\n",
       " 2515.0,\n",
       " 83.0,\n",
       " 84.0,\n",
       " 0.8,\n",
       " 51.0,\n",
       " 19.0,\n",
       " 224.87,\n",
       " 0.8888888888888888,\n",
       " 0.9166666666666666,\n",
       " 18.0,\n",
       " 24.81,\n",
       " 44.0,\n",
       " 50.0,\n",
       " 387.85,\n",
       " 1.0,\n",
       " 48.0,\n",
       " 151.0,\n",
       " 6029.0,\n",
       " 55.0,\n",
       " 0.625,\n",
       " 88.0,\n",
       " 40.17,\n",
       " 35.0,\n",
       " 48.0,\n",
       " 5.666666666666667,\n",
       " 89.0,\n",
       " 212.0,\n",
       " 50.0,\n",
       " 16.0,\n",
       " 270.0,\n",
       " 3.25,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 69.0,\n",
       " 512.0,\n",
       " 12.0,\n",
       " 66.0,\n",
       " 15.1,\n",
       " 1460.0,\n",
       " 0.3,\n",
       " 14696.0,\n",
       " 1792.0,\n",
       " 1.0,\n",
       " 65.0,\n",
       " 74.0,\n",
       " 0.6,\n",
       " 63.0,\n",
       " 3.0,\n",
       " 12.0,\n",
       " 79.0,\n",
       " 7.26,\n",
       " 0.5,\n",
       " 12.3,\n",
       " 7.0,\n",
       " 11.0,\n",
       " 63.0,\n",
       " 6.5,\n",
       " 82.0,\n",
       " 6.0,\n",
       " 44.0,\n",
       " 0.7,\n",
       " 20.52,\n",
       " 5.0,\n",
       " 61.0,\n",
       " 67.0,\n",
       " 10.46,\n",
       " 227.0,\n",
       " 5.0,\n",
       " 48781.0,\n",
       " 11.0,\n",
       " 0.9,\n",
       " 12.0,\n",
       " 971639.0,\n",
       " 14720.0,\n",
       " 12.0,\n",
       " 21.95,\n",
       " 0.08,\n",
       " 10.333333333333332,\n",
       " 11.0,\n",
       " 0.9166666666666666,\n",
       " 5.0,\n",
       " 9.0,\n",
       " 74.0,\n",
       " 3.0,\n",
       " 0.75,\n",
       " 1232.0,\n",
       " 13.0,\n",
       " 0.8333333333333334,\n",
       " 6.0,\n",
       " 21.0,\n",
       " 16.0,\n",
       " 0.6666666666666667,\n",
       " 3.0,\n",
       " 0.5,\n",
       " 0.1,\n",
       " 103.0,\n",
       " 559.0,\n",
       " 42.0,\n",
       " 48.0,\n",
       " 19.02,\n",
       " 27.0,\n",
       " 106491.0,\n",
       " 12.0,\n",
       " 0.8333333333333334,\n",
       " 163.28,\n",
       " 6.0,\n",
       " 3731.0,\n",
       " 0.6666666666666666,\n",
       " 5935.0,\n",
       " 25.31,\n",
       " 16.0,\n",
       " 70.0,\n",
       " 9.12,\n",
       " 51.0,\n",
       " 0.3,\n",
       " 118548.0,\n",
       " 0.3333333333333333,\n",
       " 8.0,\n",
       " 0.31,\n",
       " 6279.0,\n",
       " 50.0,\n",
       " 71.0,\n",
       " 574.0,\n",
       " 12.0,\n",
       " 0.8333333333333334,\n",
       " 0.625,\n",
       " 37.0,\n",
       " 141.54,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 52.0,\n",
       " 56.0,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333335,\n",
       " 0.5,\n",
       " 14.12,\n",
       " 9.0,\n",
       " 2713.0,\n",
       " 6.0,\n",
       " 1.22,\n",
       " 16.0,\n",
       " 14.0,\n",
       " 23.0,\n",
       " 7.0,\n",
       " 4.333333333333333,\n",
       " 17.0,\n",
       " 24.0,\n",
       " 19766.0,\n",
       " 6755.0,\n",
       " 9.0,\n",
       " 11687.0,\n",
       " 0.6,\n",
       " 12.7,\n",
       " 72.0,\n",
       " 4.0,\n",
       " 10.666666666666666,\n",
       " 3.0,\n",
       " 469.0,\n",
       " 1.7,\n",
       " 0.3333333333333333,\n",
       " 9.8,\n",
       " 0.9,\n",
       " 18.0,\n",
       " 9.8,\n",
       " 0.9166666666666667,\n",
       " 37.0,\n",
       " 4133.0,\n",
       " 6.0,\n",
       " 59.0,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 47.0,\n",
       " 29.0,\n",
       " 100.0,\n",
       " 342.0,\n",
       " 13.0,\n",
       " 0.7,\n",
       " 0.16666666666666666,\n",
       " 2.0,\n",
       " 0.4,\n",
       " 77.0,\n",
       " 34.0,\n",
       " 252.0,\n",
       " 3.5,\n",
       " 5.0,\n",
       " 18.8,\n",
       " 0.25,\n",
       " 4.0,\n",
       " 17.0,\n",
       " 3120.0,\n",
       " 0.6,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 111421.0,\n",
       " 105.0,\n",
       " 16.0,\n",
       " 78.0,\n",
       " 5.0,\n",
       " 215.0,\n",
       " 13.0,\n",
       " 6.0,\n",
       " 17.0,\n",
       " 104.0,\n",
       " 10.0,\n",
       " 0.52,\n",
       " 41.0,\n",
       " 15.0,\n",
       " 65057.0,\n",
       " 211.0,\n",
       " 26.58,\n",
       " 72.0,\n",
       " 5.333333333333333,\n",
       " 0.75,\n",
       " 8317.0,\n",
       " 13.0,\n",
       " 16.0,\n",
       " 0.8,\n",
       " 0.375,\n",
       " 47.0,\n",
       " 0.46,\n",
       " 1.3333333333333335,\n",
       " 27.3,\n",
       " 0.1,\n",
       " 0.125,\n",
       " 34.72,\n",
       " 0.6,\n",
       " 63.0,\n",
       " 4.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 89.0,\n",
       " 22.0,\n",
       " 77.0,\n",
       " 0.625,\n",
       " 39.0,\n",
       " 10.0,\n",
       " 4.0,\n",
       " 227.0,\n",
       " 16.0,\n",
       " 26.0,\n",
       " 2.75,\n",
       " 12.0,\n",
       " 33.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 43.0,\n",
       " 6.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 4.0,\n",
       " 0.7,\n",
       " 1.3333333333333333,\n",
       " 5855.0,\n",
       " 2.0,\n",
       " 0.09,\n",
       " 242.0,\n",
       " 6.0,\n",
       " 81.0,\n",
       " 83.0,\n",
       " 96.0,\n",
       " 34.0,\n",
       " 50870.0,\n",
       " 67082.0,\n",
       " 6.5,\n",
       " 67.0,\n",
       " 18.0,\n",
       " 3220.0,\n",
       " 0.7,\n",
       " 43.0,\n",
       " 4.6,\n",
       " 523.0,\n",
       " 18.0,\n",
       " 60.0,\n",
       " 57.0,\n",
       " 0.6666666666666666,\n",
       " 27004.0,\n",
       " 0.2,\n",
       " 0.75,\n",
       " 0.2,\n",
       " 55.0,\n",
       " 33.56,\n",
       " 29.0,\n",
       " 0.75,\n",
       " 74.0,\n",
       " 23.86,\n",
       " 0.25,\n",
       " 11.0,\n",
       " 0.2,\n",
       " 452.0,\n",
       " 4.0,\n",
       " 0.5,\n",
       " 488.0,\n",
       " 158.35,\n",
       " 98.0,\n",
       " 10.0,\n",
       " 46.0,\n",
       " 9.0,\n",
       " 0.375,\n",
       " 7.0,\n",
       " 4.0,\n",
       " 90.0,\n",
       " 5703.0,\n",
       " 15.0,\n",
       " 9844.0,\n",
       " 64.0,\n",
       " 217.0,\n",
       " 14.0,\n",
       " 65.0,\n",
       " 63.0,\n",
       " 20.0,\n",
       " 21.93,\n",
       " 117.0,\n",
       " 9.8,\n",
       " 53.0,\n",
       " 40.0,\n",
       " 4.0,\n",
       " 52.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 63.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 17.0]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cffa91e4-a612-4c7a-bb35-7f3dcc136fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0 25.0\n",
      "0.6666666666666666 0.666666666666666\n",
      "14.12 5.88\n",
      "Accuracy: 0.9922279792746114\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "corr = 0\n",
    "incorr = 0\n",
    "for i, j in zip(result, truth):\n",
    "    if i == j:\n",
    "        corr+=1\n",
    "    else:\n",
    "        incorr+=1\n",
    "        print(i, j)\n",
    "\n",
    "print('Accuracy:', corr/(corr+incorr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "426bd917-4cbc-4dd7-a5e4-1c80b92ed137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea9346-b94a-4640-9fee-154405f54430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
