{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8947e2c-a06e-4f07-a676-f1ea41f1565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training, AdaLoraConfig, AdaLoraConfig\n",
    "\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15178bec-c1a7-4c59-8bc0-73a69bd9e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets  = load_dataset(\"glue\", 'stsb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e387ff91-2c51-4337-825b-a732cbd40547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.hidden_dropout_prob=0.0\n",
    "config.attention_probs_dropout_prob=0.00\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c36a1a-afa5-4e03-8f81-c67fad5aaa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e0d8351b5f4bef81fe6a2be0d37b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "col_to_delete = ['sentence1','sentence2']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True,max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a497dd-196a-4fef-8296-9807c7ed057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8977910b-c948-4208-a6b5-74e3e9620dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f275066c-3a5f-42d4-b1bb-5ed2f24559a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_alpha = 5 #16\n",
    "lora_dropout = 0.05 #0.1\n",
    "lora_rank = 4 #64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"query\",\n",
    "        \"key\",\n",
    "        \"value\",\n",
    "        \"dense\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ae80e-7a76-49dc-8a39-e16c9a67280b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e712870-8165-40df-a94b-63e9ed11b8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a6f043-4961-4b12-9738-42ef33dfbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0f8d1d-2b84-477d-b5a0-4f757d49f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "def compute_metrics(pred):\n",
    "    preds = np.squeeze(pred.predictions)\n",
    "    return {\"MSE\": ((preds - pred.label_ids) ** 2).mean().item(),\n",
    "            \"RMSE\": (np.sqrt (( (preds - pred.label_ids) ** 2).mean())).item(),\n",
    "            \"MAE\": (np.abs(preds - pred.label_ids)).mean().item(),\n",
    "            \"Pearson\" : pearsonr(preds,pred.label_ids)[0],\n",
    "            \"Spearman's Rank\":spearmanr(preds,pred.label_ids)[0]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da7fdb7-64f7-4b1f-ace1-59b59035042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='qnli_dir',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.00,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62ff1f5-60ed-4e61-b5b6-71158a37b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 04:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman's rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.278600</td>\n",
       "      <td>8.005220</td>\n",
       "      <td>8.005220</td>\n",
       "      <td>2.829350</td>\n",
       "      <td>2.399258</td>\n",
       "      <td>0.043047</td>\n",
       "      <td>0.048613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.041000</td>\n",
       "      <td>2.776332</td>\n",
       "      <td>2.776331</td>\n",
       "      <td>1.666233</td>\n",
       "      <td>1.356969</td>\n",
       "      <td>0.705864</td>\n",
       "      <td>0.710354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.220100</td>\n",
       "      <td>0.894615</td>\n",
       "      <td>0.894615</td>\n",
       "      <td>0.945841</td>\n",
       "      <td>0.756487</td>\n",
       "      <td>0.835816</td>\n",
       "      <td>0.844573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>0.599473</td>\n",
       "      <td>0.599473</td>\n",
       "      <td>0.774256</td>\n",
       "      <td>0.593198</td>\n",
       "      <td>0.870994</td>\n",
       "      <td>0.874984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.799237</td>\n",
       "      <td>0.799237</td>\n",
       "      <td>0.894001</td>\n",
       "      <td>0.684643</td>\n",
       "      <td>0.868856</td>\n",
       "      <td>0.877953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.540700</td>\n",
       "      <td>0.698345</td>\n",
       "      <td>0.698345</td>\n",
       "      <td>0.835670</td>\n",
       "      <td>0.638912</td>\n",
       "      <td>0.884733</td>\n",
       "      <td>0.890167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.510100</td>\n",
       "      <td>0.693990</td>\n",
       "      <td>0.693990</td>\n",
       "      <td>0.833061</td>\n",
       "      <td>0.636787</td>\n",
       "      <td>0.890224</td>\n",
       "      <td>0.893163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>0.624321</td>\n",
       "      <td>0.624321</td>\n",
       "      <td>0.790140</td>\n",
       "      <td>0.590578</td>\n",
       "      <td>0.885236</td>\n",
       "      <td>0.894225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.572517</td>\n",
       "      <td>0.572517</td>\n",
       "      <td>0.756649</td>\n",
       "      <td>0.587421</td>\n",
       "      <td>0.898288</td>\n",
       "      <td>0.898325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.484265</td>\n",
       "      <td>0.484265</td>\n",
       "      <td>0.695891</td>\n",
       "      <td>0.524983</td>\n",
       "      <td>0.904748</td>\n",
       "      <td>0.903571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.537413</td>\n",
       "      <td>0.537413</td>\n",
       "      <td>0.733085</td>\n",
       "      <td>0.559902</td>\n",
       "      <td>0.901427</td>\n",
       "      <td>0.901160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>0.498306</td>\n",
       "      <td>0.498306</td>\n",
       "      <td>0.705908</td>\n",
       "      <td>0.526386</td>\n",
       "      <td>0.902065</td>\n",
       "      <td>0.902935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.445940</td>\n",
       "      <td>0.445940</td>\n",
       "      <td>0.667787</td>\n",
       "      <td>0.508449</td>\n",
       "      <td>0.904101</td>\n",
       "      <td>0.903278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.463939</td>\n",
       "      <td>0.463939</td>\n",
       "      <td>0.681130</td>\n",
       "      <td>0.509089</td>\n",
       "      <td>0.905183</td>\n",
       "      <td>0.903029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.452745</td>\n",
       "      <td>0.452745</td>\n",
       "      <td>0.672863</td>\n",
       "      <td>0.505925</td>\n",
       "      <td>0.905077</td>\n",
       "      <td>0.903631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.443875</td>\n",
       "      <td>0.443875</td>\n",
       "      <td>0.666240</td>\n",
       "      <td>0.502272</td>\n",
       "      <td>0.906522</td>\n",
       "      <td>0.904757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.460575</td>\n",
       "      <td>0.460575</td>\n",
       "      <td>0.678657</td>\n",
       "      <td>0.510107</td>\n",
       "      <td>0.905862</td>\n",
       "      <td>0.904412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.461352</td>\n",
       "      <td>0.461352</td>\n",
       "      <td>0.679229</td>\n",
       "      <td>0.510667</td>\n",
       "      <td>0.905788</td>\n",
       "      <td>0.904263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1800, training_loss=1.2607858159806993, metrics={'train_runtime': 285.085, 'train_samples_per_second': 201.659, 'train_steps_per_second': 6.314, 'total_flos': 2123636283570294.0, 'train_loss': 1.2607858159806993, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8579a-32bc-4416-b680-cad2fd0d2c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fb79a-8eb0-4856-84bc-899000925332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
