{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", 'stsb')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.hidden_dropout_prob=0.1\n",
    "config.attention_probs_dropout_prob=0.01\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = DebertaV2ForQuestionAnswering.from_pretrained(\"/data2/nusrat/work/bert-finetuned-squad_2/checkpoint-47500\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "# col_to_delete = ['idx']\n",
    "col_to_delete = ['sentence1','sentence2']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True,max_length=512)\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(preprocessing_function, batched=True)\n",
    "\n",
    "# tokenized_test_dataset = test_dataset.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "# tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "# Define the custom linear layer\n",
    "class PropulsionLinear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True, degree=15, **kwargs):\n",
    "        super(PropulsionLinear, self).__init__()\n",
    "        # Initialize the underlying nn.Linear with both the specified arguments and any additional kwargs\n",
    "        self.linear = nn.Linear(input_features, output_features, bias=bias, **kwargs)\n",
    "        self.propulsion = nn.Parameter(torch.ones(output_features))\n",
    "        self.degree = degree\n",
    " \n",
    "    def forward(self, x):\n",
    "        push = torch.pow(self.propulsion, self.degree)\n",
    "        return torch.mul(self.linear(x), push)\n",
    "    \n",
    "class PropulsionEmbedding(nn.Module):\n",
    "    def __init__(self, degree=15, **kwargs):\n",
    "        super(PropulsionEmbedding, self).__init__()\n",
    "        # Initialize the embedding layer with kwargs passed to the constructor\n",
    "        self.embeddings = nn.Embedding(**kwargs)\n",
    "        # Assuming embedding_dim is one of the kwargs, use it to initialize propulsion\n",
    "        self.propulsion = nn.Parameter(torch.ones(kwargs['embedding_dim']))\n",
    "        self.degree = degree\n",
    "        \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.embeddings.weight\n",
    "    \n",
    "    def forward(self, x):\n",
    "        push = torch.pow(self.propulsion, self.degree)\n",
    "        return torch.mul(self.embeddings(x), push)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PropulsionLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, degree=1, **kwargs):\n",
    "        super(PropulsionLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape, **kwargs)\n",
    "        self.propulsion = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.degree = degree\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.push = torch.pow(self.propulsion, self.degree)\n",
    "        return self.layer_norm(x)* self.push\n",
    "\n",
    "def replace_layers_with_custom(model, linear_degree=55, embedding_degree=55):\n",
    "    \"\"\"\n",
    "    Recursively replaces nn.Linear and nn.Embedding layers with CustomLinear\n",
    "    and CustomEmbedding layers, copying the weights and setting the degrees.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        # Replace nn.Linear with CustomLinear\n",
    "        if isinstance(module, nn.Linear) and (name == 'query' or name == 'value' or name ==\"key\"):\n",
    "            custom_linear = custom_linear = PropulsionLinear(module.in_features, module.out_features, module.bias is not None, degree=linear_degree)\n",
    "            custom_linear.linear.weight = nn.Parameter(module.weight.data.clone())\n",
    "            if module.bias is not None:\n",
    "                custom_linear.linear.bias = nn.Parameter(module.bias.data.clone())\n",
    "            setattr(model, name, custom_linear)\n",
    "        # Replace nn.Embedding with CustomEmbedding\n",
    "        elif isinstance(module, nn.Embedding) and (name == 'dquery' or name == 'dvalue'):\n",
    "            custom_embedding = PropulsionEmbedding(num_embeddings=module.num_embeddings, embedding_dim=module.embedding_dim, padding_idx=module.padding_idx, degree=embedding_degree)\n",
    "            custom_embedding.embeddings.weight = nn.Parameter(module.weight.data.clone())\n",
    "            setattr(model, name, custom_embedding)\n",
    "\n",
    "        else:\n",
    "            # Recursively apply this function to children modules\n",
    "            replace_layers_with_custom(module, linear_degree=linear_degree, embedding_degree=embedding_degree)\n",
    "\n",
    "\n",
    "# Load a pretrained BERT model\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, config = config)\n",
    "replace_layers_with_custom(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: classifier.dense.weight, Shape: torch.Size([768, 768])\n",
      "Parameter name: classifier.dense.bias, Shape: torch.Size([768])\n",
      "Parameter name: classifier.out_proj.weight, Shape: torch.Size([1, 768])\n",
      "Parameter name: classifier.out_proj.bias, Shape: torch.Size([1])\n",
      "Total trainable parameters:591361, percentage:  0.004743256553735573\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers by default\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze specific layers by name\n",
    "for name, param in model.named_parameters():\n",
    "    if 'intermediate.dense.propulsion' in name or 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "# Count of trainable parameters\n",
    "total_trainable_params = 0\n",
    "total =  0\n",
    "# Print trainable parameters and count their total number\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter name: {name}, Shape: {param.shape}\")\n",
    "        \n",
    "        total_trainable_params += param.numel()\n",
    "    total+=param.numel()\n",
    "\n",
    "print(f\"Total trainable parameters:{total_trainable_params}, percentage:  {total_trainable_params/total}\")\n",
    "\n",
    "#677378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "def compute_metrics(pred):\n",
    "    preds = np.squeeze(pred.predictions)\n",
    "    return {\"MSE\": ((preds - pred.label_ids) ** 2).mean().item(),\n",
    "            \"RMSE\": (np.sqrt (( (preds - pred.label_ids) ** 2).mean())).item(),\n",
    "            \"MAE\": (np.abs(preds - pred.label_ids)).mean().item(),\n",
    "            \"Pearson\" : pearsonr(preds,pred.label_ids)[0],\n",
    "            \"Spearman's Rank\":spearmanr(preds,pred.label_ids)[0]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nprottas/miniconda3/envs/up/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-17 23:29:44,953] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nprottas/miniconda3/envs/up/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/3600 00:25 < 02:39, 19.46 it/s, Epoch 1.39/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman's rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.172100</td>\n",
       "      <td>3.954603</td>\n",
       "      <td>3.954603</td>\n",
       "      <td>1.988618</td>\n",
       "      <td>1.656765</td>\n",
       "      <td>-0.091175</td>\n",
       "      <td>-0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.728700</td>\n",
       "      <td>2.443684</td>\n",
       "      <td>2.443684</td>\n",
       "      <td>1.563229</td>\n",
       "      <td>1.348247</td>\n",
       "      <td>-0.092732</td>\n",
       "      <td>-0.121381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.808400</td>\n",
       "      <td>2.285645</td>\n",
       "      <td>2.285645</td>\n",
       "      <td>1.511835</td>\n",
       "      <td>1.303083</td>\n",
       "      <td>-0.093683</td>\n",
       "      <td>-0.132030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.570700</td>\n",
       "      <td>2.332458</td>\n",
       "      <td>2.332458</td>\n",
       "      <td>1.527239</td>\n",
       "      <td>1.298349</td>\n",
       "      <td>-0.089221</td>\n",
       "      <td>-0.127007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.570500</td>\n",
       "      <td>2.307018</td>\n",
       "      <td>2.307018</td>\n",
       "      <td>1.518887</td>\n",
       "      <td>1.295494</td>\n",
       "      <td>-0.083525</td>\n",
       "      <td>-0.123462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='qnli_dir',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.0,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 05:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman's rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.523347</td>\n",
       "      <td>0.523347</td>\n",
       "      <td>0.723427</td>\n",
       "      <td>0.544805</td>\n",
       "      <td>0.897757</td>\n",
       "      <td>0.897671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.517710</td>\n",
       "      <td>0.517710</td>\n",
       "      <td>0.719521</td>\n",
       "      <td>0.539356</td>\n",
       "      <td>0.893852</td>\n",
       "      <td>0.896360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.533262</td>\n",
       "      <td>0.533262</td>\n",
       "      <td>0.730248</td>\n",
       "      <td>0.550391</td>\n",
       "      <td>0.895557</td>\n",
       "      <td>0.897318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.480085</td>\n",
       "      <td>0.480085</td>\n",
       "      <td>0.692881</td>\n",
       "      <td>0.525105</td>\n",
       "      <td>0.897591</td>\n",
       "      <td>0.897120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.407200</td>\n",
       "      <td>0.477549</td>\n",
       "      <td>0.477549</td>\n",
       "      <td>0.691049</td>\n",
       "      <td>0.515289</td>\n",
       "      <td>0.897742</td>\n",
       "      <td>0.896342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.557626</td>\n",
       "      <td>0.557626</td>\n",
       "      <td>0.746743</td>\n",
       "      <td>0.558162</td>\n",
       "      <td>0.897534</td>\n",
       "      <td>0.895501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.729383</td>\n",
       "      <td>0.552290</td>\n",
       "      <td>0.896476</td>\n",
       "      <td>0.893685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.484052</td>\n",
       "      <td>0.484052</td>\n",
       "      <td>0.695739</td>\n",
       "      <td>0.519285</td>\n",
       "      <td>0.896699</td>\n",
       "      <td>0.896244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.451589</td>\n",
       "      <td>0.451589</td>\n",
       "      <td>0.672003</td>\n",
       "      <td>0.498328</td>\n",
       "      <td>0.897960</td>\n",
       "      <td>0.897129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.471176</td>\n",
       "      <td>0.471176</td>\n",
       "      <td>0.686423</td>\n",
       "      <td>0.505446</td>\n",
       "      <td>0.898620</td>\n",
       "      <td>0.896015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>0.483849</td>\n",
       "      <td>0.483849</td>\n",
       "      <td>0.695593</td>\n",
       "      <td>0.513924</td>\n",
       "      <td>0.897688</td>\n",
       "      <td>0.897549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.507308</td>\n",
       "      <td>0.507308</td>\n",
       "      <td>0.712256</td>\n",
       "      <td>0.526285</td>\n",
       "      <td>0.897784</td>\n",
       "      <td>0.897444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.493812</td>\n",
       "      <td>0.493812</td>\n",
       "      <td>0.702717</td>\n",
       "      <td>0.524970</td>\n",
       "      <td>0.899222</td>\n",
       "      <td>0.899126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>0.702356</td>\n",
       "      <td>0.521904</td>\n",
       "      <td>0.899985</td>\n",
       "      <td>0.898804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.463680</td>\n",
       "      <td>0.463680</td>\n",
       "      <td>0.680941</td>\n",
       "      <td>0.502043</td>\n",
       "      <td>0.899804</td>\n",
       "      <td>0.897501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.345100</td>\n",
       "      <td>0.501976</td>\n",
       "      <td>0.501976</td>\n",
       "      <td>0.708502</td>\n",
       "      <td>0.527246</td>\n",
       "      <td>0.897054</td>\n",
       "      <td>0.898457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>0.506305</td>\n",
       "      <td>0.506305</td>\n",
       "      <td>0.711551</td>\n",
       "      <td>0.529573</td>\n",
       "      <td>0.900008</td>\n",
       "      <td>0.898667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.488938</td>\n",
       "      <td>0.488938</td>\n",
       "      <td>0.699241</td>\n",
       "      <td>0.528137</td>\n",
       "      <td>0.901877</td>\n",
       "      <td>0.899692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.351300</td>\n",
       "      <td>0.489905</td>\n",
       "      <td>0.489905</td>\n",
       "      <td>0.699932</td>\n",
       "      <td>0.525336</td>\n",
       "      <td>0.899911</td>\n",
       "      <td>0.898861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.487600</td>\n",
       "      <td>0.487600</td>\n",
       "      <td>0.698283</td>\n",
       "      <td>0.524742</td>\n",
       "      <td>0.899658</td>\n",
       "      <td>0.898874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.339800</td>\n",
       "      <td>0.479962</td>\n",
       "      <td>0.479962</td>\n",
       "      <td>0.692793</td>\n",
       "      <td>0.512580</td>\n",
       "      <td>0.900509</td>\n",
       "      <td>0.898167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.540211</td>\n",
       "      <td>0.540211</td>\n",
       "      <td>0.734990</td>\n",
       "      <td>0.556763</td>\n",
       "      <td>0.901262</td>\n",
       "      <td>0.898776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.527590</td>\n",
       "      <td>0.527590</td>\n",
       "      <td>0.726354</td>\n",
       "      <td>0.550348</td>\n",
       "      <td>0.900295</td>\n",
       "      <td>0.898159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0.472014</td>\n",
       "      <td>0.472014</td>\n",
       "      <td>0.687033</td>\n",
       "      <td>0.516333</td>\n",
       "      <td>0.900942</td>\n",
       "      <td>0.899783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>0.506421</td>\n",
       "      <td>0.506421</td>\n",
       "      <td>0.711633</td>\n",
       "      <td>0.541722</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.898941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.659755</td>\n",
       "      <td>0.488488</td>\n",
       "      <td>0.900202</td>\n",
       "      <td>0.897798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.475380</td>\n",
       "      <td>0.475380</td>\n",
       "      <td>0.689478</td>\n",
       "      <td>0.514761</td>\n",
       "      <td>0.899586</td>\n",
       "      <td>0.898475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>0.475384</td>\n",
       "      <td>0.475384</td>\n",
       "      <td>0.689481</td>\n",
       "      <td>0.513499</td>\n",
       "      <td>0.900669</td>\n",
       "      <td>0.898056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.459003</td>\n",
       "      <td>0.459003</td>\n",
       "      <td>0.677498</td>\n",
       "      <td>0.498244</td>\n",
       "      <td>0.900062</td>\n",
       "      <td>0.897924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.461468</td>\n",
       "      <td>0.461468</td>\n",
       "      <td>0.679314</td>\n",
       "      <td>0.500584</td>\n",
       "      <td>0.899845</td>\n",
       "      <td>0.897887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.502940</td>\n",
       "      <td>0.502940</td>\n",
       "      <td>0.709182</td>\n",
       "      <td>0.534917</td>\n",
       "      <td>0.900682</td>\n",
       "      <td>0.898443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.456414</td>\n",
       "      <td>0.456414</td>\n",
       "      <td>0.675584</td>\n",
       "      <td>0.501738</td>\n",
       "      <td>0.900842</td>\n",
       "      <td>0.898542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.485022</td>\n",
       "      <td>0.485022</td>\n",
       "      <td>0.696435</td>\n",
       "      <td>0.522853</td>\n",
       "      <td>0.900772</td>\n",
       "      <td>0.898498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.691868</td>\n",
       "      <td>0.515982</td>\n",
       "      <td>0.900786</td>\n",
       "      <td>0.898531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>0.493746</td>\n",
       "      <td>0.493746</td>\n",
       "      <td>0.702671</td>\n",
       "      <td>0.527319</td>\n",
       "      <td>0.900685</td>\n",
       "      <td>0.898351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.486010</td>\n",
       "      <td>0.486010</td>\n",
       "      <td>0.697145</td>\n",
       "      <td>0.522027</td>\n",
       "      <td>0.900616</td>\n",
       "      <td>0.898254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3600, training_loss=0.3436892943912082, metrics={'train_runtime': 305.3379, 'train_samples_per_second': 188.283, 'train_steps_per_second': 11.79, 'total_flos': 1876859850634716.0, 'train_loss': 0.3436892943912082, 'epoch': 10.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
